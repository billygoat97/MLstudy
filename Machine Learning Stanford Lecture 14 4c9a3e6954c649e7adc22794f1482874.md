# Machine Learning Stanford Lecture 14

Class: CS229
Created: Aug 12, 2020 12:47 AM
Reviewed: Yes

Unsupervised Learning

→ 정보를 가공해야~ 우리가 쓸수 있음!

K means clustering → 군집화의 일부 알고리즘

cluster centroids  → 중간

호오

윤호햄

여기에 오다니 무슨일인가ㅋㅋ염탐실패

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled.png)

x ⇒ centroid

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%201.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%201.png)

 new centroid → 남은 걸로 다시 또 centroid찾기 ——-

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%202.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%202.png)

하나로 converge하게 됨!

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%203.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%203.png)

이 방식으로 하나의 점으로 converge하게 됩니당.

for every i → color the points

for each j → move the cluster centroid

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%204.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%204.png)

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%205.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%205.png)

c → assignments, mu ⇒ centroids

k means  is exactly coordinate descent on J. Specifically, the
inner-loop of k-means repeatedly minimizes J with respect to c while holding
µ fixed, and then minimizes J with respect to µ while holding c fixed. Thus,
J must monotonically decrease, and the value of J must converge.

# Density estimation

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%206.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%206.png)

초록색 → anomaly detection

군집 속할 확률 낮다 → something is wrong!

# Mixtures of Gaussians

data set

one row number → comes from 2 gaussian

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%207.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%207.png)

problem → which gaussian?

Suppose there's a latent (hidden/unobserved)

random variable z, and x(i), z(i) are distributed

P(x(i),z(i)) = P(x(i)|z(i))P(z(i))

where z(i) N multinomial (phi)

k = 

x(i) was generated by randomly choosing z(i) from{1, . . . , k}, and then x(i) was drawn from one of k Gaussians depending on z(i). This is called the mixture of Gaussians model.

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%208.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%208.png)

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%209.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%209.png)

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2010.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2010.png)

z의 값을 몰라서 못씀 ㅠ

그렇다면?

# E.M (expectaion maximation) 2 step

1. E step (guess value of z(i)'s  (soft guess)

In the E-step, we calculate the posterior probability of our parameters
the z(i)’s, given the x(i) 

and using the current setting of our parameters

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2011.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2011.png)

2. M step

assume first part guess is true

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2012.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2012.png)

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2013.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2013.png)

airplane engine plant 1,2 → 80%, 20% which engine got from plant 1 or 2

each guess, E step,  M step look at all engine → 확률 up, weight 증가, 아니면 weight 감소 10%라면 낮춤 이런식으로 조절하는 것

# Jensen's inequality

f = convex function

이계도함수가 >0

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2014.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2014.png)

x = 1 with probability 1/2

x = 5 with prob 1/2

f(EX) ≤ E[f(x)] always

f(EX) = E[f(x)] ↔ X is a constant

if f = concave function,

이계도 함수 < 0

f(EX) ≥ E[f(x)] always

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2015.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2015.png)

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2016.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2016.png)

maximize l($/theta$)

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2017.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2017.png)

e, find maximum of cur, m move it to equation → eventually, 최상값으로 이동

# 이쪽부분 이해가 잘 안됨 다시 볼 것.

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2018.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2018.png)

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2019.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2019.png)

f(Ex) ≥ E[f(x)]

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2020.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2020.png)

c = constant

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2021.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2021.png)

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2022.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2022.png)

E step

Q(z(i)) := P(z(i)|x(i);theta)

M step

![Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2023.png](Machine%20Learning%20Stanford%20Lecture%2014%204c9a3e6954c649e7adc22794f1482874/Untitled%2023.png)